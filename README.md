# ğŸ” Large Language Models as Cryptanalysts  
**Decrypting Classical Ciphers with AI**  



## ğŸ“œ Research Overview  
**Title:** *Large Language Models as Cryptanalysts: Assessing Decryption Capabilities Across Classical Ciphers*  
**Institution:** Department of Electrical and Computer Engineering, American University of Beirut  

### ğŸ‘¥ Authors  
| Name            | Contact                     |  
|-----------------|-----------------------------|  
| Zainab Saad     | zas31@mail.aub.edu          |  
| Hadi Tfaily     | hht08@mail.aub.edu          |  
| Aline Hassan    | afh29@mail.aub.edu          |  

---

## ğŸ¯ Key Objectives  
âœ” Evaluate LLM decryption accuracy for classical ciphers  
âœ” Compare zero-shot vs. fine-tuned transformer performance  
âœ” Develop enhanced positional embedding architectures  
âœ” Assess implications for modern cryptographic security  

---

## ğŸ” Investigated Ciphers  
| Cipher           | Type          | Challenge Level |  
|------------------|---------------|------------------|  
| **Caesar**       | Substitution  | â˜…â˜†â˜†â˜†â˜†            |  
| **Monoalphabetic**| Substitution | â˜…â˜…â˜†â˜†â˜†            |  
| **VigenÃ¨re**     | Polyalphabetic| â˜…â˜…â˜…â˜†â˜†            |  
| **Rail Fence**   | Transposition | â˜…â˜…â˜†â˜†â˜†            |  

---

## ğŸ—ï¸ Technical Architecture  
```mermaid
graph TD
    A[Input Ciphertext] --> B(Preprocessing)
    B --> C{Model Type?}
    C -->|Zero-shot| D[LLM Inference]
    C -->|Fine-tuned| E[Custom Transformer]
    D & E --> F[Decrypted Plaintext]
```

**Core Components:**  
- PyTorch-based transformer models  
- Optuna hyperparameter optimization  
- Mixed-precision (FP16) training  
- Gradient checkpointing for memory efficiency  

---

## âš™ï¸ Implementation  
### ğŸ“¦ Dependencies  
```python
pip install torch transformers optuna streamlit gdown
```

### ğŸš€ Quick Start  
1. Clone repository  
2. Install requirements:  
   ```bash
   pip install -r requirements.txt
   ```  
3. Launch interactive demo:  
   ```bash
   streamlit run test_cmps.py
   ```

---

## ğŸ“Š Evaluation Framework  
| Metric               | Measurement Method          |  
|----------------------|-----------------------------|  
| Token-Level Accuracy | Exact match percentage      |  
| Character Accuracy   | Levenshtein distance        |  
| Latency              | Inference time (ms/token)   |  
| Length Robustness    | Accuracy vs. sequence length|  

---

## ğŸ“‚ Repository Structure  
```
.
â”œâ”€â”€ models/            # Pretrained weights
â”œâ”€â”€ data/              # Training datasets
â”œâ”€â”€ src/               # Core implementations
â”‚   â”œâ”€â”€ architectures.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ test_cmps.py   # Interactive demo
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“œ Citation  
If referencing this work, please use:  
```bibtex
@misc{llm_cryptanalysis2024,
  title={Large Language Models as Cryptanalysts},
  author={Saad, Zainab and Tfaily, Hadi and Hassan, Aline},
  year={2024},
  institution={American University of Beirut}
}
```

---

