# ğŸ” Large Language Models as Cryptanalysts

**Title:** Large Language Models as Cryptanalysts: Assessing Decryption Capabilities Across Classical Ciphers  
**Authors:**  
- Zainab Saad â€” zas31@mail.aub.edu  
- Hadi Tfaily â€” hht08@mail.aub.edu  
- Aline Hassan â€” afh29@mail.aub.edu  
**Institution:** Department of Electrical and Computer Engineering, American University of Beirut

---

## ğŸ§  Project Overview

This project explores the ability of **Large Language Models (LLMs)**â€”such as Mistral 7B and other transformer-based architecturesâ€”to decrypt classical encryption schemes, including:

- Caesar Cipher  
- Monoalphabetic Cipher  
- VigenÃ¨re Cipher  
- Rail Fence Cipher  

We evaluate both **zero-shot**, **few-shot**, and **fully fine-tuned** transformer models for their decryption capabilities. The research investigates whether LLMs can function as cryptanalysts and what implications that has for modern cryptography and cybersecurity.

---

## ğŸ¯ Objectives

- Evaluate the **decryption accuracy** of LLMs across classical ciphers.
- Train **vanilla transformers** and **enhanced positional models** specifically for decryption tasks.
- Explore the performance of **zero-shot vs fine-tuned** LLMs.
- Study the implications of AI-based cryptanalysis for future cryptographic security.

---

## ğŸ“š Ciphers Covered

| Cipher Type        | Description                                                   |
|--------------------|---------------------------------------------------------------|
| **Caesar Cipher**  | Each letter is shifted by a fixed number of positions.        |
| **Monoalphabetic** | Each letter maps to another using a fixed random permutation. |
| **VigenÃ¨re Cipher**| Uses a keyword to apply a series of Caesar shifts.            |
| **Rail Fence**     | A transposition cipher writing text in zigzag rails.          |

---

## ğŸ—ï¸ Architecture

- Fine-tuned transformer models using **PyTorch**
- Enhanced positional embeddings for Caesar cipher
- Modular configuration system for easy experimentation
- Training on Google Colab with **NVIDIA A100/L4 GPUs**
- Optimization using **Optuna**, mixed precision (FP16), and gradient checkpointing

---

## ğŸ“¦ Libraries and Tools

- **PyTorch** â€” Model architecture & training
- **Optuna** â€” Hyperparameter tuning
- **Hugging Face Transformers** â€” Model & tokenizer support
- **Streamlit** â€” Web-based demo for live decryption
- **gdown** â€” Download model weights from Google Drive

---

## ğŸ§ª Evaluation Metrics

- **Token-Level Accuracy** â€” Exact match rate per token
- **Character-Level Accuracy** â€” Fine-grained match accuracy
- **Decryption Latency** â€” Time taken to produce output
- **Sequence Length Effects** â€” Impact of longer input texts

---

## ğŸ“ Project Structure

â”œâ”€â”€ streamlit_app.py # Streamlit demo interface
â”œâ”€â”€ models/ # Folder for pretrained models (downloaded on demand)
â”œâ”€â”€ data/ # Datasets used for training/fine-tuning
â”œâ”€â”€ src/ # Core model architectures and utilities
â”œâ”€â”€ README.md # This documentation
â”œâ”€â”€ requirements.txt # Python dependencies


---

## ğŸš€ Running the Streamlit App

You can run the interactive decryption demo locally using Streamlit.

### 1. Install Dependencies

```bash
pip install -r requirements.txt

---
##  Start the Streamlit App

streamlit run streamlit_app.py
